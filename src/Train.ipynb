{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "da922ca6-9207-48bd-8c25-bfb77f5a209d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "%run config.ipynb\n",
    "%run InkML-parser.ipynb\n",
    "%run tokenizer.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fdde860a-f5d3-4df6-98f8-85b0bcea109a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torchinfo import summary\n",
    "from pprint import pprint\n",
    "from transformers import ViTConfig, ViTModel\n",
    "from transformers import MT5Config, MT5Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "12f1f74f-2ce7-4f0d-bf79-2c9c23c25d5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ViT Model Configuration: ViTConfig {\n",
      "  \"_attn_implementation_autoset\": true,\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"encoder_stride\": 16,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 512,\n",
      "  \"image_size\": 224,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1024,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"model_type\": \"vit\",\n",
      "  \"num_attention_heads\": 8,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"patch_size\": 16,\n",
      "  \"qkv_bias\": true,\n",
      "  \"transformers_version\": \"4.47.0\"\n",
      "}\n",
      "\n",
      "=========================================================================================================\n",
      "Layer (type:depth-idx)                                  Output Shape              Param #\n",
      "=========================================================================================================\n",
      "ViTModel                                                [64, 512]                 --\n",
      "├─ViTEmbeddings: 1-1                                    [64, 197, 512]            101,376\n",
      "│    └─ViTPatchEmbeddings: 2-1                          [64, 196, 512]            --\n",
      "│    │    └─Conv2d: 3-1                                 [64, 512, 14, 14]         393,728\n",
      "│    └─Dropout: 2-2                                     [64, 197, 512]            --\n",
      "├─ViTEncoder: 1-2                                       [64, 197, 512]            --\n",
      "│    └─ModuleList: 2-3                                  --                        --\n",
      "│    │    └─ViTLayer: 3-2                               [64, 197, 512]            2,102,784\n",
      "│    │    └─ViTLayer: 3-3                               [64, 197, 512]            2,102,784\n",
      "│    │    └─ViTLayer: 3-4                               [64, 197, 512]            2,102,784\n",
      "│    │    └─ViTLayer: 3-5                               [64, 197, 512]            2,102,784\n",
      "│    │    └─ViTLayer: 3-6                               [64, 197, 512]            2,102,784\n",
      "│    │    └─ViTLayer: 3-7                               [64, 197, 512]            2,102,784\n",
      "├─LayerNorm: 1-3                                        [64, 197, 512]            1,024\n",
      "├─ViTPooler: 1-4                                        [64, 512]                 --\n",
      "│    └─Linear: 2-4                                      [64, 512]                 262,656\n",
      "│    └─Tanh: 2-5                                        [64, 512]                 --\n",
      "=========================================================================================================\n",
      "Total params: 13,375,488\n",
      "Trainable params: 13,375,488\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (Units.GIGABYTES): 5.76\n",
      "=========================================================================================================\n",
      "Input size (MB): 38.54\n",
      "Forward/backward pass size (MB): 2891.97\n",
      "Params size (MB): 53.10\n",
      "Estimated Total Size (MB): 2983.60\n",
      "=========================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Define ViT model \n",
    "vitConfig = ViTConfig(\n",
    "    image_size=IMG_SIZE,\n",
    "    patch_size=PATCH_SIZE,\n",
    "    num_channels=IMG_IN_CHANNELS,\n",
    "    hidden_size=D_MODEL,\n",
    "    num_hidden_layers=VIT_N_LAYERS,\n",
    "    num_attention_heads=VIT_N_HEADS,\n",
    "    intermediate_size=VIT_FFN_HIDDEN,\n",
    "    hidden_dropout_prob=VIT_DROPOUT,\n",
    "    attention_probs_dropout_prob=VIT_DROPOUT,\n",
    ")\n",
    "\n",
    "ViT_model = ViTModel(vitConfig)\n",
    "\n",
    "logger.info(f'ViT Model Configuration: {ViT_model.config}')\n",
    "print(f'ViT Model Configuration: {ViT_model.config}')\n",
    "\n",
    "logger.info(summary(ViT_model, input_size=(BATCH_SIZE, IMG_IN_CHANNELS, IMG_SIZE, IMG_SIZE)))\n",
    "print(summary(ViT_model, input_size=(BATCH_SIZE, IMG_IN_CHANNELS, IMG_SIZE, IMG_SIZE)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9191a347-a7ed-4589-ac4d-578542f3f6a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ViT Model Configuration: MT5Config {\n",
      "  \"_attn_implementation_autoset\": true,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 1024,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"mt5\",\n",
      "  \"num_decoder_layers\": 3,\n",
      "  \"num_heads\": 8,\n",
      "  \"num_layers\": 3,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"tokenizer_class\": \"T5Tokenizer\",\n",
      "  \"transformers_version\": \"4.47.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 1002\n",
      "}\n",
      "\n",
      "==============================================================================================================\n",
      "Layer (type:depth-idx)                                       Output Shape              Param #\n",
      "==============================================================================================================\n",
      "MT5Model                                                     [64, 512, 512]            --\n",
      "├─MT5Stack: 1-1                                              [64, 512, 512]            8,381,184\n",
      "├─MT5Stack: 1-2                                              --                        (recursive)\n",
      "│    └─Embedding: 2-1                                        [64, 512, 512]            513,024\n",
      "├─MT5Stack: 1-3                                              --                        (recursive)\n",
      "│    └─Dropout: 2-2                                          [64, 512, 512]            --\n",
      "│    └─ModuleList: 2-3                                       --                        --\n",
      "│    │    └─MT5Block: 3-1                                    [64, 512, 512]            2,622,720\n",
      "│    │    └─MT5Block: 3-2                                    [64, 512, 512]            2,622,464\n",
      "│    │    └─MT5Block: 3-3                                    [64, 512, 512]            2,622,464\n",
      "│    └─MT5LayerNorm: 2-4                                     [64, 512, 512]            512\n",
      "│    └─Dropout: 2-5                                          [64, 512, 512]            --\n",
      "├─MT5Stack: 1-4                                              [64, 8, 512, 64]          513,024\n",
      "│    └─Embedding: 2-6                                        [64, 512, 512]            (recursive)\n",
      "│    └─Dropout: 2-7                                          [64, 512, 512]            --\n",
      "│    └─ModuleList: 2-8                                       --                        --\n",
      "│    │    └─MT5Block: 3-4                                    [64, 512, 512]            3,671,808\n",
      "│    │    └─MT5Block: 3-5                                    [64, 512, 512]            3,671,552\n",
      "│    │    └─MT5Block: 3-6                                    [64, 512, 512]            3,671,552\n",
      "│    └─MT5LayerNorm: 2-9                                     [64, 512, 512]            512\n",
      "│    └─Dropout: 2-10                                         [64, 512, 512]            --\n",
      "==============================================================================================================\n",
      "Total params: 28,290,816\n",
      "Trainable params: 28,290,816\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (Units.GIGABYTES): 1.27\n",
      "==============================================================================================================\n",
      "Input size (MB): 0.52\n",
      "Forward/backward pass size (MB): 11442.06\n",
      "Params size (MB): 77.59\n",
      "Estimated Total Size (MB): 11520.17\n",
      "==============================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Define mT5 Model \n",
    "mt5Config = MT5Config(\n",
    "    vocab_size=ENC_NUM_TOKENS,\n",
    "    d_model=D_MODEL,\n",
    "    num_layers=MT5_ENC_N_LAYERS,\n",
    "    num_decoder_layers=MT5_DEC_N_LAYERS,\n",
    "    num_heads=MT5_ENC_N_HEADS,\n",
    "    d_ff=MT5_FFN_HIDDEN,\n",
    "    dropout_rate=MT5_DROPOUT,\n",
    ")\n",
    "\n",
    "MT5_model = MT5Model(mt5Config)\n",
    "\n",
    "logger.info(f'ViT Model Configuration: {MT5_model.config}')\n",
    "print(f'ViT Model Configuration: {MT5_model.config}')\n",
    "\n",
    "tmp_mt5_input_data = {\n",
    "    \"input_ids\": torch.randint(0, ENC_NUM_TOKENS, (BATCH_SIZE, MT5_ENC_MAX_SEQ_LEN)),\n",
    "    \"decoder_input_ids\": torch.randint(0, DEC_NUM_TOKENS, (BATCH_SIZE, MT5_DEC_MAX_SEQ_LEN))\n",
    "\n",
    "}\n",
    "\n",
    "logger.info(summary(MT5_model, input_data=tmp_mt5_input_data))\n",
    "print(summary(MT5_model, input_data=tmp_mt5_input_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8c9c37-ea62-473c-b794-9631ff959035",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
